classdef DatabaseAnalysis < handle & DataSource & Cacheable

    properties(SetAccess=?Cacheable)
        % time which the analysis started running
        timeRun

        % has this analysis run already?
        hasRun = false;

        % is the analysis currently running? used by internal functions
        % to allow error-free calling of runOnEntry outside of .run()
        isRunning = false;
        
        % is the analysis currently running in a mode where results will be
        % saved
        isSavingResults = true;

        % the result table will be an instance of DatabaseAnalysisResultsTable
        resultTable

        % will be populated post analysis. All of this info is in .resultTable as well
%         successByEntry
%         exceptionByEntry
%         logByEntry

        % each element contains a struct with fields
        %   .pathNoExt
        %   .fileNameNoExt
        %   .extensions
        %   .name
        %   .caption
        %   .width
        %   .height
        %figureInfoByEntry = {};

        % there are two modes in which the results of this analysis can persist.
        % if cacheResultsIndividually is false, the results will be saved with
        % the table, ,,j
        %cacheResultsIndividually = false;
    end

    properties
        figureExtensions = {'fig', 'png', 'svg', 'pdf'};

        runDescription = ''; % added to the top of each report
    end

    properties(SetAccess=protected, Transient)
        % used internally when calling methods on the analysis
        % from within runOnEntry (e.g. saveFigure)
        figureInfoCurrentEntry
        currentEntry
        
        tableMapped
        
        % reference to the current database when running
        database
    end

    properties(Dependent)
        fieldsAnalysis

        pathAnalysisRoot
        pathAnalysis
        pathCurrent
        pathFigures
        htmlFile
    end

    methods(Abstract) % METHODS EVERY ANALYSIS MUST IMPLEMENT
        % return the entryName corresponding to the table in the database which this
        % analysis runs on. The DataTable with this entry name will run this analysis
        % once on each entry and map the results via a 1-1 relationship
        entryName = getMapsEntryName(da);

        % run this analysis on one entry, entryTable will be a DataTable instance
        % filtered down to one entry
        resultStruct = runOnEntry(da, entry, fields)
    end

    methods % not necessary to override if the defaults are okay
        
        % return a list of fields generated by the analysis. These need to be declared
        % ahead of time to simplify many of the caching related features.
%         [fields, fieldDescriptorMap] = getFieldsAnalysis(da);
%         [fieldStructToFieldDescriptor] = getFieldsAnalysis(da);
        function fields = getFieldsAnalysis(da)
            fields = struct();
        end
        
        % return a single word descriptor for this analysis, ignoring parameter
        % settings in param. The results will be stored as a DataTable with this
        % as the entryName. Default is lower cased version of analysis
        % class name
        function name = getName(da)
            fullClass = class(da);
            % strip package name
            idxdot = strfind(fullClass, '.');
            if ~isempty(idxdot)
                fullClass = fullClass(idxdot(end)+1:end);
            end
                
            name = lowerFirst(fullClass);
        end
        
        % return the parameters that describe this particular instance of the analysis
        % these will be used when caching. If these are changed, the old
        % analysis results will no longer be found
        function param = getCacheParam(da)
            param = struct();
        end
        
        function hash = generateHashForEntry(da, entry, hash, cacheName, cacheParam)
            % by default, keep hash as it is, but analysis can choose to redefine the hash however it wants
            % taking on the risk of non-uniqueness. A common overwrite would be to use entry.getKeyFieldValueDescriptors() {1}
            % and then factor in param 
        end
        
        function prefix = getCacheFilePrefix(da)
            prefix = 'cache_';
        end
        
        function lookup = getCustomCacheSuffixForFieldLookup(da)
            lookup = struct();
        end

        % return a string used to describe the params used for t]his analysis
        % should encompass whatever is returned by getCacheParam()
        function str = getDescriptionParam(da)
            str = structToString(da.getCacheParam(), '; ');
        end
        
        % if true, this will be a run-once type analysis where all entries
        % are passed in and all results are returned simultaneously. All
        % other aspects are the same, so the results will be saved into
        % different files as usual
        function tf = getRunOnceOnAllEntriesSimultaneously(da)
            tf = false;
        end

        % return fields here that you wish to have custom control over the
        % save load process
        function fields = getFieldsCustomSaveLoad(da, other)
            fields = {};
        end

        function data = loadValuesCustomForEntry(da, entry, fields, extraInfo) %#ok<STOUT>
            % here entry will be the mapped entry, not the
            error('Please override loadValuesCustomForEntry if you wish to use getFieldsCustomSaveLoad');
        end

        function saveValuesCustomForEntry(da, entry, data, extraInfo)
            error('Please override saveValuesCustomForEntry if you wish to use getFieldsCustomSaveLoad');
        end

        % determine if this DatabaseAnalysis instance is equivalent to other,
        % an instance which has already been loaded in the Database.
        function tf = isequal(da, other)
            tf = true;
            if ~strcmp(class(da), class(other))
                tf = false;
                return;
            end
            if ~isequal(da.getCacheParam(), other.getCacheParam())
                tf = false;
                return;
            end
        end

        function tf = getCacheFieldsIndividually(da) %#ok<*MANU>
            tf = false;
        end

        function tf = getRerunCachedUnsuccessful(da)
            tf = false;
        end

        % returns a list of entryNames that this analysis references
        % this is used when retrieving analysis results from the cache. If a related
        % table has changed, the analysis will be rerun on all entries.
        function list = getReferencesRelatedEntryNames(da)
            list = {};
        end

        % An analysis runs on each entry of a specific data table, and may reference
        % related tables through relationships in the database. When any of these
        % data tables is modified, this could invalidate the results of an analysis
        % for some or all of the entries. However, for caching to be at all useful
        % we simply issue a warning when using cached analysis results that references
        % a datatable that has changed. We assume that all changes have been additive
        % and do not affect the analysis that has already been run. Thus we only
        % run the analysis on entries that are missing related rows in this analysis.
        %
        % However, if this will yield incorrect results, you can specify that
        % when certain tables are changed at all, the entire cached analysis
        % becomes invalid. Return a list of the .entryName of these tables here.
        function list = getEntryNamesChangesInvalidateCache(da)
            list = {};
        end

        % return a cell array of DataSource instances that must be loaded a priori,
        % These sources will ALWAYS be run, even when all analysis can be loaded
        % from cache. If a source is needed only when doing new analysis, e.g.
        % another analysis this builds upon, include it in getRequiredSourcesForAnalysis
        % instead.
        function sources = getRequiredSources(da)
            sources = {};
        end

        % return a cell array of DataSource instances that must be loaded ONLY
        % when new analysis is to actually be run (not just loading from cache)
        function sources = getRequiredSourcesForAnalysis(da)
            sources = {};
        end

        % return a cell array of DatabaseView instances that must be applied before
        % running this analysis
        function views = getRequiredViews(da)
            views = {};
        end

        % filter the data table as necessary before this analysis is run
        % table.entryName will match the result of getMapsEntryName() above
        %
        % If the filtering pattern is a common one, consider turning it into a
        % DatabaseView class and returning an instance from getDatabaseViewsToApply()
        function [table, changed] = preFilterTable(da, table) %#ok<*INUSL>
            % default does nothing
            changed = false;
        end
        
        % runs after analysis is loaded in the database (as a DataSource)
        function postLoadInDatabase(da, table)
            
        end

        % return a list of additional meta fields that resultTable will contain
        % in addition to the analysis field and keyFields
        function [fields, fieldDescriptorMap] = getFieldsAdditional(da, table) %#ok<*INUSD>
            fieldDescriptorMap = ValueMap('KeyType', 'char', 'ValueType', 'any');
            fieldDescriptorMap('success') = BooleanField();
            fieldDescriptorMap('output') = OutputField();
            fieldDescriptorMap('runTimestamp') = DateTimeField();
            fieldDescriptorMap('exception') = UnspecifiedField();
            fieldDescriptorMap('figureInfo') = UnspecifiedField();
            fields = fieldDescriptorMap.keys;
        end

        % return the list of fields to load when the analysis is loaded
        % into the database as a DataSource via .loadSource()
        function fieldsToLoad = getFieldsToLoadOnDataSourceLoad(da)
            % by default load all displayable fields
            fieldsToLoad = {'success', 'runTimestamp', 'output', 'exception'};

            if ~isempty(da.resultTable)
                fieldsToLoad = union(fieldsToLoad, ...
                    intersect(da.resultTable.fieldsLoadOnDemand, da.resultTable.fieldsDisplayable));
            end
        end

    end

    methods % Constructor
        function da = DatabaseAnalysis(db)
            if nargin > 0
                da.setDatabase(db);
            end
        end
    end

    methods
        function setDatabase(da, db)
            %assert(~da.hasRun, 'Database cannot be changed after analysis has run');
            assert(isempty(db) || isa(db, 'Database'), 'Must be a Database instance');
            da.database = db;
        end

        % originally i defined getFieldsAnalysis to use a value map, then I
        % decided to switch to a struct format. this provides easy
        % compatibility
        function [fields, map] = getFieldsAnalysisAsValueMap(da)
            try
                [fields, map] = da.getFieldsAnalysis();
            catch
                fstruct = da.getFieldsAnalysis();
                fields = fieldnames(fstruct);
                map = ValueMap.fromStruct(fstruct);
            end
        end

        function checkHasRun(da)
            if ~da.hasRun
                error('Analysis has not yet been run. Please call .run() first.');
            end
        end

        function checkIsRunning(da)
            if ~da.getIsRunning()
                error('Analysis is not currently running. Please call .run() first.');
            end
        end

        function tf = getIsRunning(da)
            tf = da.isRunning || da.hasRun;
        end

        function readyDatabase(da)
            % set up in database
            if isempty(da.database)
                error('Please call .setDatabase(db) first');
            end

            % load all data sources
            da.database.removeSourceWithName(da.getName()); % important to get rid of conflicting sources
            da.database.loadSource(da.getRequiredSources());

            % load all data views
            da.database.applyView(da.getRequiredViews());
        end

        function initialize(da, varargin)
            p = inputParser;
            p.addParameter('maxRows', Inf, @isscalar);
            p.parse(varargin{:});

            name = da.getName();
            assert(isvarname(name), 'getName() must return a valid variable name');
            debug('Initializing DatabaseAnalysis %s\n', name);

            da.readyDatabase();

%             currentResultTable = da.resultTable;

            % build the resultTable as a LoadOnDemandTable
            % this will be a skeleton containing all of the fields for the analysis
            % none of which will be loaded initially
            resultTable = DatabaseAnalysisResultsTable(da, 'table', da.tableMapped, 'maxRows', p.Results.maxRows);

            % commenting this out as it does not deal properly with
            % changing hash parameters!
%             if ~isempty(currentResultTable)
%                 debug('Merging current results table into newly mapped results table, some entries may be dropped\n');
%                 resultTable = currentResultTable.mergeEntriesWith(resultTable, 'keyFieldMatchesOnly', true);
%             end

            % don't want to filter OneToOne relatyionships in case we've
            % subselected the entries to run on already
            da.resultTable = resultTable.setDatabase(da.database).updateInDatabase('filterOneToRelationships', false);
            entryName = da.getMapsEntryName();

            % empty entry name means runs once on entire database
            if ~isempty(entryName)
                da.database.addRelationshipOneToOne(da.resultTable.entryName, entryName);
            end

            da.database.markSourceLoaded(da);
        end

        function runDebug(da, varargin)
            % wraps .run with common params for debugging but won't
            % actually do any saving
            dbstop if error;
            da.run('keepCurrentValues', false, 'loadCache', false, ...
                'saveCache', false, 'catchErrors', false, 'rerunFailed', true, varargin{:});
        end
        
        function runDebugKeepCurrent(da, varargin)
            dbstop if error;
            da.run('keepCurrentValues', true, 'loadCache', false, ...
                'saveCache', false, 'catchErrors', false, 'rerunFailed', true, varargin{:});
        end

        function runDebugErrors(da, varargin)
            % wraps .run with common params for debugging but will actually
            % do work if nothing goes wrong
            dbstop if error;
            da.run('keepCurrentValues', false, 'loadCache', true, ...
                'saveCache', true, 'catchErrors', false, 'rerunFailed', true, varargin{:});
        end

        function rerun(da, varargin)
            % wraps .run with common params for debugging, i.e. don't
            % saveCache, loadCache, catchErrors, but do rerunFailed
            da.run('keepCurrentValues', false, 'loadCache', false, ...
                'rerunFailed', true, varargin{:});
        end

        function rerunDebugErrors(da, varargin)
            % wraps .run with common params for debugging but will actually
            % do work if nothing goes wrong
            dbstop if error;
            da.run('keepCurrentValues', false, 'loadCache', false, ...
                'saveCache', true, 'catchErrors', false, varargin{:});
        end

        function rerunFailed(da, varargin)
            da.run('rerunFailed', true, varargin{:});
        end

        function rerunFailedDebugErrors(da, varargin)
            % wraps .run with common params for debugging but will actually
            % do work if nothing goes wrong
            dbstop if error;
            da.run('rerunFailed', true, 'saveCache', true, 'catchErrors', false, varargin{:});
        end

        function rerunFailedDebug(da, varargin)
            dbstop if error;
            da.run('rerunFailed', true, 'catchErrors', false, 'saveCache', false, varargin{:});
        end

        function runDebugOnIdx(da, idx, varargin)
            da.runDebug('idx', idx, varargin{:});
        end

        function run(da, varargin)
            p = inputParser();
            p.addParameter('desc', '', @ischar);
            p.addParameter('database', [], @(db) isempty(db) || isa(db, 'Database'));
            % optionally select subset of fields for analysis
            p.addParameter('fields', da.fieldsAnalysis, @iscellstr);
            % check the cache for existing analysis values
            p.addParameter('loadCache', true, @islogical);
            % load the cached success value so we know which entries have been run
            p.addParameter('loadCacheSuccessOnly', false, @islogical);
            % look at cached field value timestamps and invalidate them if they
            % are newer than the most recent modification to any table returned
            % by getEntryNamesChangesInvalidateCache()
            p.addParameter('checkCacheTimestamps', true, @islogical);
            % save computed analysis values to the cache
            p.addParameter('saveCache', true, @islogical);
            % rerun any failed entries from prior runs, true value supersedes
            % .getRerunFailed() method return value, which is the class default
            p.addParameter('rerunFailed', false, @islogical);
            p.addParameter('recheckSuccess', true, @islogical); %set to false if you've just loaded all the success values
            % don't run any new analysis, just load whatever possible from the cache
            p.addParameter('loadCacheOnly', false, @islogical);
            % wrap the runOnEntry method in a try/catch block so that errors
            % on one entry don't halt the analysis
            p.addParameter('catchErrors', true, @islogical);
            % generate a new report and figures folder even if no new analysis
            % was run, useful if issues encountered with report generation
            p.addParameter('forceReport', false, @islogical);
            p.addParameter('writeReport', true, @islogical);
            
            % limit the indices that are run from the table
            p.addParameter('idx', [], @(x) isempty(x) || isvector(x));
            
            p.addParameter('runOnEntries', [], @(x) isempty(x) || isa(x, 'DataTable')); % manually override which entries we run on

            % will drop the current results table before loading new values
            % if true, otherwise will merge it in
            p.addParameter('keepCurrentValues', false, @islogical);

            % limit this for debugging to make things go faster initially
            p.addParameter('maxRows', Inf, @isscalar);

            % for debug purposes mainly, run only on this many entries
            p.addParameter('maxToRun', Inf, @isscalar);

            % for new entries that run, they will be unloaded from the table immediately
            % if this is false so as to prevent memory overflows for large analyses
            % if true, new analysis results will be kept in the table,
            % but old results will still need to be loaded using
            % loadFields. Note that if saveCache is false, storeInTable
            % will always be overridden to true so that the results are
            % accessible.
            p.addParameter('storeInTable', false, @islogical);

            p.addParameter('verbose', false, @islogical);
            
            p.addParameter('parallel', false, @islogical); % use parallel mode to execute
            p.addParameter('runArgs', {}, @iscell);
            p.KeepUnmatched = true;
            p.parse(varargin{:});

            fieldsAnalysis = p.Results.fields;
            loadCache = p.Results.loadCache;
            loadCacheSuccessOnly = p.Results.loadCacheSuccessOnly;
            checkCacheTimestamps = p.Results.checkCacheTimestamps;
            saveCache = p.Results.saveCache;
            rerunFailed = p.Results.rerunFailed;
            loadCacheOnly = p.Results.loadCacheOnly;
            catchErrors = p.Results.catchErrors;
            forceReport = p.Results.forceReport;
            writeReport = p.Results.writeReport;
            storeInTable = p.Results.storeInTable;
            keepCurrentValues = p.Results.keepCurrentValues;
            maxToRun = p.Results.maxToRun;
            db = p.Results.database;
            verbose = p.Results.verbose;
            runParallel = p.Results.parallel;
            runArgs = p.Results.runArgs;

            runOnAllSimultaneously = da.getRunOnceOnAllEntriesSimultaneously();
            
            if runOnAllSimultaneously && runParallel
                error('Parallel mode not supported when getRunOnceOnAllEntriesSimultaneously() returns true');
            end
            
            if runParallel
                pool = gcp;
            end
            
            if ~saveCache
                storeInTable = true;
            end

            % get analysis name
            name = da.getName();
            debug('Preparing for analysis : %s\n', name);

            if ~isempty(db)
                da.setDatabase(db);
            end
            db = da.database;

            % call this before calling preFilterTable below!
            da.readyDatabase();

            % get the appropriate table to map
            entryName = da.getMapsEntryName();
            if isempty(entryName)
                table = StructTable(struct(), 'entryName', 'database');
            elseif ~isempty(p.Results.runOnEntries)
                table = p.Results.runOnEntries;
                assert(strcmp(table.entryName, entryName), 'runOnEntries must be DataTable with entryName %s', entryName);
            else
                table = db.getTable(entryName);
                % enforce singular for 1:1 relationship to work
                entryName = table.entryName;
            end
            
            if ~isempty(entryName)
                % prefilter the table further if requested (database views also do this)
                debug('Filtering mapped table %s via preFilterTable\n', entryName);
                [table, changed] = da.preFilterTable(table);
                if changed
                    table = table.updateInDatabase();
                end
            end
            
            da.tableMapped = table;

            % load required source/views, re-map the result table, etc.
            if ~keepCurrentValues || isempty(da.resultTable)
                da.resultTable = [];
                da.initialize('maxRows', p.Results.maxRows);
            end
            resultTable = da.resultTable; %#ok<*PROPLC>

            da.isRunning = true;
            da.isSavingResults = saveCache;

            % mark run timestamp consistently for all entries
            % we'll keep this timestamp unless we don't end up doing any new
            % analysis, then we'll just pick the most recent prior timestamp
            da.timeRun = now;
            
            allFieldsAnalysis = da.getFieldsAnalysisAsValueMap();
%             [allFieldsAnalysis, allDFDAnalysis] = da.getFieldsAnalysisAsValueMap();
%             for iField = 1:length(fieldsAnalysis)
%                 dfd = allDFDAnalysis(fieldsAnalysis{iField});
%                 fieldsAnalysisIsDisplayable(iField) = dfd.isDisplayable();
%             end

            % keep track of whether we need to re-cache the result table
            resultTableChanged = false;

            if maxToRun < resultTable.nEntries
                debug('Selecting only first %d entries to run on\n', maxToRun);
                resultTable = resultTable.select(1:maxToRun);
            end

            % mask by entry of which entries must be run
            maskAlreadyRun = false(resultTable.nEntries, 1);
            maskFailed = false(resultTable.nEntries, 1);
            maskToAnalyze = true(resultTable.nEntries, 1);
            maskCacheInvalidates = false(resultTable.nEntries, 1);

            tableListCacheWarning = makecol(da.getReferencesRelatedEntryNames());
            tableListCacheInvalidate = makecol(da.getEntryNamesChangesInvalidateCache());

            cacheFieldsIndividually = da.getCacheFieldsIndividually();

            if loadCacheSuccessOnly
                % only loading the success field, not running any new
                % entries, used primarly by DataSource.loadSource

                debug('Loading cached success field for all entries\n');
                % load success so that we can check failed entries later
                resultTable = resultTable.loadFields('fields', {'success'}, 'loadCacheOnly', true, 'verbose', verbose);
                resultTable = resultTable.updateInDatabase();
                loadCache = false;
                loadCacheOnly = true;
                timestampsByEntry = cell2mat(resultTable.cacheTimestampsByEntry);

                if checkCacheTimestamps && (~isempty(tableListCacheWarning) || ~isempty(tableListCacheInvalidate))
                    % warn about checking cache timestamps
                    debug('Warning: not checking cache timestamps for validity against relevant tables returned by getReferencesRelatedEntryNames (%s) or getEntryNamesChangesInvalidate (%s)\n', ...
                        strjoin(tableListCacheWarning, ','), strjoin(tableListCacheInvalidate, ','));
                end

                successFlag = resultTable.success;
                for iEntry = 1:resultTable.nEntries
                    if isempty(timestampsByEntry(iEntry).success)
                        maskAlreadyRun(iEntry) = false;

                    else
                        maskAlreadyRun(iEntry) = true;
                        if ~successFlag(iEntry)
                            % success field found, but failed last time

                            % if it has a success field and it's marked as
                            % failed, consider it loaded. We'll rerunFailed
                            % below if requested
                            maskFailed(iEntry) = true;
                        else
                            maskFailed(iEntry) = false;
                        end
                    end
                end

                debug('Valid cached success field found for %d of %d entries\n', nnz(maskAlreadyRun), length(maskAlreadyRun));
                debug('Previous run failed on %d of these %d cached entries\n', nnz(maskFailed), nnz(maskAlreadyRun));
            end

            % here we ask resultTable to check cache existence and timestamps
            % for all entries in the table. Timestamps are checked against
            if loadCache
                % load the table itself from cache and copy over additional field
                % values from the cache hit if present
                %if resultTable.hasCache()
                    % LoadOnDemandMappedTable will automatically add all rows in
                    % resultTable that are missing in the cached copy (i.e. rows
                    % in the mapped table that were added after the cache was generated).
                    %
                    % So we don't need to worry about adding these missing rows

                    % IMPORTANT
                    % for now, all fields are cached to disk, so this step is unnecessary
                    % uncomment this if any fields are cached with the table
                    % because then loadFromCache will need to be called to retrieve
                    % these values. All fields currently are loaded by the .loadFields()
                    % call below.
                    %
                    % resultTable = resultTable.loadFromCache();
                %end

                % check for modifications to related tables that should invalidate
                % the cached results. Entries with cached fields older than the
                % most recent modification to the related entry names will be
                % re run. The list of table entryNames to check is returned by
                % da.getEntryNamesChangesInvalidateCache()
                %
                % Also for the list of referenced entry names  returned by
                % da.getReferencesRelatedEntryNames(), generate a warning
                % if the cache is older than this value, but don't re run it

                % check the cache timestamps to determine
                % which entry x field cells are missing or out of date
                if ~keepCurrentValues
                    if p.Results.recheckSuccess
                        debug('Checking cached field existence and success field\n');
                        resultTable = resultTable.loadFields('fields', {'success'}, 'loadCacheOnly', true, 'verbose', verbose);
                    end
                
                    fieldsToLoad = setdiff(resultTable.fieldsCacheable, 'success');
                    % load the cache timestamps (and thereby determine whether
                    % they exist) for all fields for successful entries
                    resultTable = resultTable.loadFields('fields', fieldsToLoad, 'loadCacheTimestampsOnly', true, ...
                        'entryMask', resultTable.success, 'verbose', verbose);
                    resultTable.updateInDatabase('filterOneToRelationships', false);
                end

                timestampsByEntry = cell2mat(resultTable.getValues('cacheTimestampsByEntry'));

                if checkCacheTimestamps && ~isempty(tableListCacheWarning) || ~isempty(tableListCacheInvalidate)
                    % now we search for field values in fieldsAnalysis in the cache
                    debug('Checking cached field timestamps against dependencies\n');

                    resultTable = resultTable.updateInDatabase('filterOneToRelationships', false); %#ok<*PROP>
                    % get the most recent update for each table list
                    cacheWarningReference = db.getLastUpdated(tableListCacheWarning);
                    cacheInvalidateReference = db.getLastUpdated(tableListCacheInvalidate);

                    cacheWarningEntryCount = 0;
                    for iField = 1:length(fieldsAnalysis)
                        field = fieldsAnalysis{iField};
                        for iEntry = 1:resultTable.nEntries
                            timestamp = timestampsByEntry(iEntry).(field);
                            if isempty(timestamp) || timestamp < cacheWarningReference
                                cacheWarningEntryCount = cacheWarningEntryCount + 1;
                            end
                            if isempty(timestamp) || timestamp < cacheInvalidateReference
                                maskCacheInvalidates(iEntry) = true;
                            end
                        end
                    end
                    if cacheWarningEntryCount > 0
                        debug('Warning: This DatabaseAnalysis has % entries with cached field values older than\nthe modification time of tables this analysis references (see getReferencesRelatedEntryNames()).\nUse .deleteCache() or call with ''loadCache'', false to force a full re-run.\n', ...
                            cacheWarningEntryCount);
                    end
                    if any(maskCacheInvalidates)
                        debug('Warning: This DatabaseAnalysis has % entries with cached field values older than\nthe modification time of tables this analysis depends upon (see getEntryNamesChangesInvalidatesCache()).\nUse .deleteCache() or call with ''loadCache'', false to force a full re-run.\n', ...
                            nnz(maskCacheInvalidates));
                    end
                end

                % now we determine which entries have fully cached field values
                % and thus need not be rerun
                successFlag = resultTable.success;
                for iEntry = 1:resultTable.nEntries
                    if isempty(timestampsByEntry(iEntry).success) || ...
                       isnan(timestampsByEntry(iEntry).success)
                        % no success field found cached, not yet run
                        maskAlreadyRun(iEntry) = false;

                    else
                        if ~successFlag(iEntry)
                            % success field found, but failed last time

                            % if it has a success field and it's marked as
                            % failed, consider it loaded. We'll rerunFailed
                            % below if requested
                            maskFailed(iEntry) = true;
                            maskAlreadyRun(iEntry) = true;

                        else
                            % successful last time, check the existence of all fields
                            allLoaded = true;
                            for iField = 1:length(fieldsAnalysis)
                                field = fieldsAnalysis{iField};
                                if isempty(timestampsByEntry(iEntry).(field))
                                    allLoaded = false;
                                    break;
                                end
                            end

                            maskFailed(iEntry) = false;
                            maskAlreadyRun(iEntry) = allLoaded;
                        end
                    end
                end

                debug('Valid cached field values found for %d of %d entries\n', nnz(maskAlreadyRun), length(maskAlreadyRun));
                debug('Previous run failed on %d of %d cached entries\n', nnz(maskFailed), nnz(maskAlreadyRun));
            end

            % reanalyze if any fields are missing, or if the cache is invalid
            maskToAnalyze = ~maskAlreadyRun | maskCacheInvalidates;

            if rerunFailed
                % rerun if failed last time
                maskToAnalyze = maskToAnalyze | maskFailed;
            end

            % pare down entries to analyze if manually specified
            if ~ismember(p.UsingDefaults, 'idx')
                maskManual = TensorUtils.vectorIndicesToMask(p.Results.idx, numel(maskToAnalyze));
                maskToAnalyze = maskToAnalyze & maskManual;
                debug('Manually sub-selecting %d entries to run due to idx parameter\n', numel(maskToAnalyze));
            end

            % NOTE: At this point you cannot assume that resultsTable and table (the mapped table)
            % are in the same order. They simply have the same number of rows mapped via
            % key field equivalence (1:1 relationship)

            % here we analyze entries that haven't been loaded from cache
            if ~loadCacheOnly
                nFailed = nnz(maskFailed);
                if nFailed > 0
                    if rerunFailed
                        debug('Rerunning on %d entries which failed last time\n', nFailed);
                    else
                        debug('Not rerunning on %d entries which failed last time\n', nFailed);
                    end
                end

                savedAutoApply = resultTable.autoApply;
                resultTable = resultTable.setAutoApply(false);

                nAnalyze = nnz(maskToAnalyze);
                if nAnalyze > 0
                    % create a failure entry we can simply drop in when a
                    % failure occurs
                    failureEntry = struct();
                    for iField = 1:numel(allFieldsAnalysis)
                        dfd = resultTable.getFieldDescriptor(allFieldsAnalysis{iField});
                        failureEntry.(allFieldsAnalysis{iField}) = dfd.getEmptyValueElement();
                    end

                    idxAnalyze = find(maskToAnalyze);

                    resultTableChanged = true;

                    debug('Running analysis %s on %d of %d %s entries\n', name, ...
                        nAnalyze, da.tableMapped.nEntries, entryName);

                    % load sources required ONLY for new analysis
                    da.database.loadSource(da.getRequiredSourcesForAnalysis());

                    % actually run the analysis, non-parallel
                    if ~runParallel
                        set(0, 'DefaultFigureWindowStyle', 'normal'); % need this in case figures are docked by default - has issues in parpool

                        if ~runOnAllSimultaneously
                            for iAnalyze = 1:nAnalyze
                                iResult = idxAnalyze(iAnalyze);
                                [valid, entry] = fetchEntry(da.tableMapped, iResult);
                                if ~valid
                                    continue;
                                end
                                printSingleEntryHeader(entry, iAnalyze, iResult)

                                % for saveFigure to look at
                                da.currentEntry = entry;

                                % clear debug's last caller info to get a fresh
                                % debug header
                                debug();

                                % open a temporary file to use as a diary to capture all output
                                diary off;
                                diaryFile = tempname();
                                diary(diaryFile);
                                DatabaseAnalysis.setOutputLogStatus('file', diaryFile);

                                [resultStruct, success, exc] = runSingleEntry(da, entry, fieldsAnalysis, catchErrors);

                                extraFields = printPostRunWarnings(da, success, fieldsAnalysis, resultStruct);

                                % load the output from the diary file
                                DatabaseAnalysis.setOutputLogStatus('file', '');
                                diary('off');
                                output = fileread(diaryFile);

                                resultTable = storeResultsInDatabase(resultTable, resultStruct, iResult, output, success, exc, extraFields, da.figureInfoCurrentEntry);

                                % don't clutter with temp files
                                if exist(diaryFile, 'file')
                                    delete(diaryFile);
                                end

                                close all;
                            end
                            
                        else
                            % call runOnEntry simultaneously on all entries 
                            entriesToAnalyze = da.tableMapped.select(idxAnalyze);
                            printHeader('Running analysis on %d %s simultaneously\n', nAnalyze, da.tableMapped.entryNamePlural);
                            
                            da.currentEntry = entriesToAnalyze.select(1);
                            
                            % clear debug's last caller info to get a fresh
                            % debug header
                            debug();

                            % open a temporary file to use as a diary to capture all output
                            diary off;
                            diaryFile = tempname();
                            diary(diaryFile);
                            DatabaseAnalysis.setOutputLogStatus('file', diaryFile);

                            [resultStruct, success, exc] = runSingleEntry(da, entriesToAnalyze, fieldsAnalysis, catchErrors);

                            assert(numel(resultStruct) == entriesToAnalyze.nEntries, 'runOnEntry must return a struct with the same size as the entry table provided');
                            
                            extraFields = printPostRunWarnings(da, success, fieldsAnalysis, resultStruct);

                            % load the output from the diary file
                            DatabaseAnalysis.setOutputLogStatus('file', '');
                            diary('off');
                            output = fileread(diaryFile);

                            prog = ProgressBar(nAnalyze, 'Storing analysis results to cache');
                            for iAnalyze = 1:nAnalyze
                                prog.update(iAnalyze);
                                iResult = idxAnalyze(iAnalyze);
                                % only store output and figure with the
                                % first entry
                                if iAnalyze == 1
                                    resultTable = storeResultsInDatabase(resultTable, resultStruct(iAnalyze), iResult, output, success, exc, extraFields, da.figureInfoCurrentEntry);
                                else
                                    resultTable = storeResultsInDatabase(resultTable, resultStruct(iAnalyze), iResult, '', success, exc, extraFields, []);
                                end
                            end
                            prog.finish();
                            
                        end
                    else
                        % parallel mode
                        
                        opts.fieldsAnalysis = fieldsAnalysis;
                        opts.cacheFieldsIndividually = cacheFieldsIndividually;
                        opts.verbose = verbose;
                        opts.failureEntry = failureEntry;

                        % place da on the workers once
                        data = struct('da', da, 'database', da.database, ...
                            'tableMapped', da.tableMapped);
                        debug('Copying database to parallel workers...\n');
                        constData = parallel.pool.Constant(data);
                        
                        analyzeOrder = randsample(numel(idxAnalyze), numel(idxAnalyze));
                        idxAnalyze = idxAnalyze(analyzeOrder);
                        
                        prog = ProgressBar(nAnalyze, 'Creating futures to run analysis on entries');
                        counter = 1;
                        for iAnalyze = 1:nAnalyze
                            prog.update(iAnalyze);
                            
                            iResult = idxAnalyze(iAnalyze);

                            futures(counter) = parfeval(pool, @asyncRunSingle, 2, constData, iResult, opts); %#ok<AGROW>
                            counter = counter + 1;
                        end
                        prog.finish();    
                       
                        % done creating futures, now waiting for them to
                        % return.
                        debug('All futures created, waiting on completion...\n');
                        for i = 1:nAnalyze
                            [iAnalyze, success, exc] = fetchNext(futures);
                            iResult = idxAnalyze(iAnalyze);
                            
                            [valid, entry] = fetchEntry(da.tableMapped, iResult);
                            if valid
                                percDone = i / nAnalyze * 100;
                                printSingleEntryHeader(entry, iAnalyze, iResult, percDone);
                            end
                            
                            out = futures(iAnalyze).Diary;
                            fprintf('%s', out);
                            
                            if ~success && ~catchErrors
                                disp(exc);
                                error('Terminating after failing on entry %d\n', iResult);
                            end
                        end
                    end
                end

                resultTable = resultTable.apply();
                resultTable = resultTable.setAutoApply(savedAutoApply);

                resultTable.updateInDatabase('filterOneToRelationships', false);

                % now fill in all of the info fields of this class with the full table data
                % these are mainly used by the DatabaseAnalysisHTMLWriter class
    %             da.successByEntry = resultTable.getValues('success') > 0;
    %             da.exceptionByEntry = resultTable.getValues('exception');
    %             da.figureInfoByEntry = resultTable.getValues('figureInfo');
    %             da.logByEntry = resultTable.getValues('output');
                da.resultTable = resultTable;

                % mark loaded in database
                da.database.markSourceLoaded(da);
                da.hasRun = true;

                if ~isempty(p.Results.desc)
                    da.runDescription = p.Results.desc;
                end

                if ~resultTableChanged && ~forceReport
                    % if we haven't run new analysis, no need to build a report
                    % so just use the timestamp from the most recent prior run
                    dfd = da.resultTable.fieldDescriptorMap('runTimestamp');
                    timeRunList = dfd.getAsDateNum(da.resultTable.getValues('runTimestamp'));
                    if ~isempty(timeRunList)
                        da.timeRun = max(timeRunList);
                    end
                elseif saveCache
                    da.loadDisplayableFields();
                    
                    smask = da.resultTable.success;
                    debug('Analysis has run successfully on %d / %d entries\n', nnz(smask), numel(smask));
                    
                    if writeReport || forceReport
                        da.createAnalysisPathAndSetPermissions();

                        % sym link figures from prior runs to the current analysis folder
                        da.linkOldFigures('saveCache', saveCache);
                        % save the html report (which will copy resources folder over too)
                        da.saveAsHtml();

                        % link from analysisName.html to index.html for easy browsing
                        da.linkHtmlAsIndex();

                        % link this timestamped directory to current for easy browsing
                        da.linkAsCurrent();
                    end
                end

                if saveCache && resultTableChanged
                    % this isn't necessary right now as all values in the table are cached
                    % separetely from the table. You must uncomment this if there are field values
                    % saved with the table in the future (as well as the section above where
                    % the resultTable is loaded from cache)
                    %
                    % Cached field values have been cached as they were generated already
                    % da.resultTable.cache('cacheValues', false);
                end

                debug('Call da.resultTable.loadFields().updateInDatabase(); to load analysis results\n');
                debug('Call da.viewAsHtml to view html report with figures and output\n');

                da.isRunning = false;
                da.isSavingResults = false;
                
                %fprintf('\n');
                debug('Finishing analysis run\n');
            end
            
            function [valid, entry] = fetchEntry(table, iResult)
                valid = false;
                % find the corresponding entry in the mapped table via the database
                % NOTE: ASSUMING THAT THE TABLES ARE ALIGNED AT
                % THIS POINT!!!
                %resultEntry = resultTable(iResult).apply();
                %entry = resultEntry.getRelated(entryName);
                entry = table.select(iResult);

                if entry.nEntries > 1
                    debug('WARNING: Multiple matches for analysis row, check uniqueness of keyField tuples in table %s. Choosing first.\n', entryName);
                    entry = entry.select(1);
                    valid = true;
                elseif entry.nEntries == 0
                    % this likely indicates a bug in building / loading resultTable from cache
                    debug('WARNING: Could not find match for resultTable row in order to do analysis');

                else
                    valid = true;
                end
            end

            function printSingleEntryHeader(entry, iAnalyze, iResult, percDone)
                if entry.nFields == 0
                    isSingleton = true;
                else
                    isSingleton = false;
                end
                description = entry.getKeyFieldValueDescriptors();
                description = description{1};
                
                if nargin < 4
                    percDone = (iAnalyze-1)/nAnalyze*100;
                end
                if isSingleton
                    printHeader('Running singleton analysis on database\n');
                else
                    printHeader('[%5.1f %% - entry %d ] Running analysis on %s\n', percDone, iResult, description);
                end
            end
            
            function printHeader(varargin)
                fprintf('\n');
                [~, width] = getTerminalSize();
                width = width(1);
                mode = getMatlabOutputMode();
                if mode == "desktop" || (isunix && ~ismac)
                    cdash = char(hex2dec('2500'));
                elseif ismac
                    cdash = char(hex2dec({'e2', '94', '80'}))';
                else
                    cdash = '-';
                end
                line = [repmat(cdash, 1, width-1) '\n'];

                color = 'bright blue';
                tcprintf(color, line);
                tcprintf(color, varargin{:});
                tcprintf(color, line);
                fprintf('\n');
            end

            function [resultStruct, success, exc] = runSingleEntry(da, entry, fieldsAnalysis, catchErrors)
                % clear the figure info for saveFigure to use
                da.figureInfoCurrentEntry = [];

                % try calling the runOnEntry callback
                if catchErrors
                    try
                        resultStruct = da.runOnEntry(entry, fieldsAnalysis, runArgs{:});
                        exc = [];
                        success = true;

                        if isempty(resultStruct)
                            resultStruct = struct();
                        end
                        if ~isstruct(resultStruct)
                            error('runOnEntry did not return a struct');
                        end
                    catch exc
                        tcprintf('red', 'EXCEPTION: %s\n', exc.getReport);
                        success = false;
                        resultStruct = struct();
                    end
                else
                    resultStruct = da.runOnEntry(entry, fieldsAnalysis, runArgs{:});
                    exc = [];
                    success = true;

                    if isempty(resultStruct)
                        resultStruct = repmat(struct(), entry.nEntries, 1); % to support run on all entries simultaneously mode
                    end
                    if ~isstruct(resultStruct)
                        error('runOnEntry did not return a struct');
                    end
                end
            end

            function extraFields = printPostRunWarnings(da, success, fieldsAnalysis, resultStruct)
                % warn if not all fields requested were returned
                % use the requested list fieldsAnalysis, a subset of dt.fieldsAnalysis
                if success
                    missingFields = setdiff(fieldsAnalysis, fieldnames(resultStruct));
                    if ~isempty(missingFields)
                        debug('WARNING: analysis on this entry did not return fields: %s\n', ...
                            strjoin(missingFields, ', '));
                    end
                    % warn if the analysis returned extraneous fields as a reminder to add them
                    % to .getFieldsAnalysis. Fields in dt.fieldsAnalysis but not fieldsAnalysis are okay
                    extraFields = setdiff(fieldnames(resultStruct), da.fieldsAnalysis);
                    if ~isempty(extraFields)
                        debug('WARNING: analysis on this entry returned extra fields not listed in .getFieldsAnalysis(): %s\n', ...
                            strjoin(extraFields, ', '));
                    end

                    tcprintf('bright green', 'Analysis ran successfully on this entry\n');
                else
                    extraFields = {};
                end
            end

            function resultTable = storeResultsInDatabase(resultTable, resultStruct, iResult, output, success, exc, extraFields, figureInfo)
                if success
                    % Copy only fieldsAnalysis that were returned.
                    % Fields in dt.fieldsAnalysis but not fieldsAnalysis are okay
                    [fieldsCopy, ~] = intersect(allFieldsAnalysis, fieldnames(resultStruct));
                else
                    % Blank all fields (even if only some were
                    % requested), and set them in the table
                    % This is to avoid conclusion or contamination
                    % with old results
                    resultStruct = failureEntry;
                    fieldsCopy = allFieldsAnalysis;
%                     fieldsReturnedMask = true(length(allFieldsAnalysis), 1);
                    extraFields = {};
                end

                if cacheFieldsIndividually
%                     fieldsCopyIsDisplayable = fieldsAnalysisIsDisplayable(fieldsReturnedMask);
                    for iF = 1:length(fieldsCopy)
                        fld = fieldsCopy{iF};
                        % don't keep any values in the table, this way we don't run out of memory as the
                        % analysis drags on
                        % Displayable field values needed for report generation will be reloaded
                        % later on in this function
                        resultTable = resultTable.setFieldValue(iResult, fld, resultStruct.(fld), ...
                            'saveCache', saveCache, 'storeInTable', storeInTable, 'verbose', verbose);
                    end
                else
                    resultEntry = rmfield(resultStruct, extraFields);
                end

                % set all of the additional field values
                if cacheFieldsIndividually
                    saveCacheIndividual = true;
                else
                    saveCacheIndividual = false;
                end

                if saveCacheIndividual || storeInTable
                    resultTable = resultTable.setFieldValue(iResult, 'success', success, 'saveCache', saveCacheIndividual, 'storeInTable', storeInTable, 'verbose', verbose);
                    resultTable = resultTable.setFieldValue(iResult, 'output', output, 'saveCache', saveCacheIndividual, 'storeInTable', storeInTable, 'verbose', verbose);
                    resultTable = resultTable.setFieldValue(iResult, 'runTimestamp', da.timeRun, 'saveCache', saveCacheIndividual, 'storeInTable', storeInTable, 'verbose', verbose);
                    resultTable = resultTable.setFieldValue(iResult, 'exception', exc, 'saveCache', saveCacheIndividual, 'storeInTable', storeInTable, 'verbose', verbose);
                    resultTable = resultTable.setFieldValue(iResult, 'figureInfo', figureInfo, 'saveCache', saveCacheIndividual, 'storeInTable', storeInTable, 'verbose', verbose);
                end

                if ~cacheFieldsIndividually
                    resultEntry.success = success;
                    resultEntry.output = output;
                    resultEntry.runTimestamp = da.timeRun;
                    resultEntry.exception = exc;
                    resultEntry.figureInfo = figureInfo;
                    resultTable = resultTable.updateEntry(iResult, resultEntry, 'saveCache', saveCache, 'storeInTable', false, 'verbose', verbose);
                end

                if ~storeInTable
                    resultTable = resultTable.unloadFieldsForEntry(iResult);
                end
            end
        end

        function saveFigure(da, figh, figName, figCaption)
            % use this to save figures while running the analysis
            if nargin < 4
                figCaption = '';
            end
            if nargin < 2
                figh = gcf;
            end
            if nargin < 3 || isempty(figName)
                figName = get(figh, 'Name');
            end

            DatabaseAnalysis.pauseOutputLog();

            drawnow;
            if isempty(da.isRunning) || ~da.isRunning
                debug('Figure %s would be saved when run via DatabaseAnalysis.run()\n', figName);
                return;
            end
            if ~da.isSavingResults
                return;
            end

            entryTable = da.currentEntry;

            assert(entryTable.nEntries == 1);

            exts = da.figureExtensions;
            nExts = length(exts);
            success = false(nExts, 1);
            fileList = cell(nExts, 1);
%             if ~feature('isdmlworker')
                tcprintf('bright cyan', 'Saving figure %s as %s\n', figName, strjoin(exts, ', '));
%             end
            for i = 1:nExts
                ext = exts{i};
                fileName = da.getFigureName(entryTable, figName, ext);
%                 fileList{i} = resolveSymLink(GetFullPath(fileName));
                fileList{i} = GetFullPath(fileName);
                mkdirRecursive(fileparts(fileList{i}));
            end

            % let saveFigure do all of the work!
            AutoAxis.updateFigure();
            drawnow;
            saveFigure(fileList, figh);

            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, fileList);

            % log figure infomration
            figInfo.name = figName;
            figInfo.caption = figCaption;
            [figInfo.width, figInfo.height] = getFigSize(figh);
            figInfo.extensions = exts;
            figInfo.fileLinkList = fileList;
            figInfo.fileList = fileList;
            figInfo.saveSuccessful = success;
            figInfo = orderfields(figInfo);

            % add to figure info cell
            if isempty(da.figureInfoCurrentEntry)
                da.figureInfoCurrentEntry = figInfo;
            else
                da.figureInfoCurrentEntry(end+1) = figInfo;
            end

            DatabaseAnalysis.resumeOutputLog();
        end

        function fileName = getFigureName(da, entryTable, figName, ext)
            % construct figure name that looks like:
            % {{analysisRoot}}/figures/ext/figName.{{keyField descriptors}}.ext

            if ischar(entryTable)
                descriptor = entryTable;
            else
                assert(entryTable.nEntries == 1);
                descriptors = entryTable.getKeyFieldValueDescriptors();
                descriptor = descriptors{1};
            end

            path = fullfile(da.pathFigures, ext);
            if isempty(descriptor)
                % when the analysis is a singleton that doesn't map a table
                fileName = fullfile(path, sprintf('%s.%s', figName, ext));
            else
                fileName = fullfile(path, sprintf('%s.%s.%s', figName, descriptor, ext));
            end
            fileName = GetFullPath(fileName);
        end

        % create a symlink to index.html
        function linkHtmlAsIndex(da)
            da.checkHasRun();
            htmlFile = da.htmlFile;
            filePath = fileparts(htmlFile);
            indexLink = fullfile(filePath, 'index.html');
            if exist(indexLink, 'file')
                cmd = sprintf('rm "%s"', indexLink);
                [status, message] = unix(cmd);
                if status
                    fprintf('Error replacing index.html symlink:\n');
                    fprintf('%s\n', message);
                end
            end
            makeSymLink(htmlFile, indexLink);
            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, indexLink);
        end

        % symlink my analysis directory to "current" for ease of navigation
        function linkAsCurrent(da)
            da.checkHasRun();
            currentPath = GetFullPath(da.pathCurrent);
            thisPath = GetFullPath(da.pathAnalysis);
            if exist(currentPath, 'dir')
                cmd = sprintf('rm "%s"', currentPath);
                [status, message] = unix(cmd);
                if status
                    fprintf('Error replacing current symlink:\n');
                    fprintf('%s\n', message);
                end
            end
            makeSymLink(thisPath, currentPath);
            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, currentPath);
        end

        % symlink all figures loaded from cache that are not saved in the same
        % directory as the most recently generated figures
        function linkOldFigures(da, varargin)
            p = inputParser;
            p.addParameter('saveCache', true, @islogical);
            p.parse(varargin{:});
%             saveCache = p.Results.saveCache;
            da.checkHasRun();

            if ~isunix && ~ismac
                % TODO add support for windows nt junctions
                return;
            end
%             figurePath = da.pathFigures;
            nEntries = da.resultTable.nEntries;

            table = da.resultTable.table;
            emptyMask = arrayfun(@(x) isempty(x.figureInfo), table);

            if any(~emptyMask)
                prog = ProgressBar(nEntries, 'Creating symbolic links to figures saved in earlier runs');

                descriptors = da.resultTable.getKeyFieldValueDescriptors();

                for iEntry = 1:nEntries
                    if emptyMask(iEntry)
                        continue;
                    end
                    info = table(iEntry).figureInfo;
                    madeChanges = false;

                    prog.update(iEntry);
                    for iFigure = 1:length(info)
                        figInfo = info(iFigure);
                        for iExt = 1:length(figInfo.extensions)
                            try
                                thisRunLocation = da.getFigureName(descriptors{iEntry}, figInfo.name, figInfo.extensions{iExt});
                                if exist(thisRunLocation, 'file')
                                    continue;
                                end

                                % point the symlink at the original file, not at the most recent link
                                % to avoid cascading symlinks
                                actualFile = figInfo.fileList{iExt};
                                success = makeSymLink(actualFile, thisRunLocation);
                                if success
                                    % change the figure info link location, not the actual file path
                                    info(iFigure).fileLinkList{iExt} = thisRunLocation;
                                    madeChanges = true;

                                    % expose permissions on the symlink and the
                                    % original file, just in case
                                    chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, {actualFile, thisRunLocation});
                                end
                            catch exc
                                debug('ERROR: linking old figure %s to %s\n', figInfo.fileLink{iExt}, thisRunLocation);
                                disp(exc.getReport);
                            end
                        end
                    end

                    % update the figure info in the result table
                    if madeChanges
                        table(iEntry).figureInfo = info;
                    end
                end

                prog.finish();

                da.resultTable.table = table;
            end

        end

        function viewAsHtml(da)
            % da.checkHasRun(); % defer to current when not run yet
            fileName = da.htmlFile;
            if ~exist(fileName, 'file')
                da.saveAsHtml();
            end
            HTMLWriter.openFileInBrowser(fileName);
        end
        
        function loadDisplayableFields(da)
            debug('Loading displayable fields for all entries\n');
            % here we're writing the report
            % before we do this, we need to load the values of all displayable fields
            % and additional fields used in the report
            fieldsToLoad = intersect(da.resultTable.fieldsLoadOnDemand, da.resultTable.fieldsDisplayable);
            fieldsAdditional_ = da.getFieldsAdditional();
            fieldsToLoad = union(fieldsToLoad, fieldsAdditional_);
            da.resultTable = da.resultTable.loadFields('fields', fieldsToLoad, 'loadCacheOnly', true, 'verbose', false);
            da.resultTable.updateInDatabase('filterOneToRelationships', false);
        end
        
        function createAnalysisPathAndSetPermissions(da)
            % make sure analysis path exists
            mkdirRecursive(da.pathAnalysis);
            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, da.pathAnalysisRoot);
            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, da.pathAnalysis);
            if exist(da.pathFigures, 'dir')
                chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, da.pathFigures);
                for i = 1:length(da.figureExtensions)
                    path = fullfile(da.pathFigures, da.figureExtensions{i});
                    if exist(path, 'dir')
                        chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, path);
                    end
                end
            end 
        end

        function html = saveAsHtml(da)
            da.checkHasRun();
            fileName = da.htmlFile;
            
            da.loadDisplayableFields();
            da.createAnalysisPathAndSetPermissions();
            
            debug('Saving HTML Report to %s\n', fileName);
            html = HTMLDatabaseAnalysisWriter(fileName);
            html.generate(da);
            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, html.fileName);
            chmod(MatdbSettingsStore.settings.permissionsAnalysisFiles, html.resourcesPathStore, 'recursive', true);
        end

        function disp(da)
            mapsStr = da.getMapsEntryName();
            if isempty(mapsStr)
                mapsStr = 'database (singleton)';
            elseif da.getRunOnceOnAllEntriesSimultaneously()
                mapsStr = [mapsStr '{yellow} (runs on all simultaneously)'];
            end
            tcprintf('inline', '{bright blue}DatabaseAnalysis {none}: {bright white}%s{none} maps {bright white}%s\n', da.getName(), mapsStr);
            tcprintf('inline', ['Parameters: ' da.getDescriptionParam() '\n\n']);
            builtin('disp', da);
        end
    end

    methods % Cacheable instantiations
        % return the cacheName to be used when instance
        function name = getCacheName(obj)
            name = [obj.getName() '_analysis'];
        end

        function timestamp = getCacheValidAfterTimestamp(obj)
            % my data is valid until the last modification timestamp of the
            if isempty(obj.database)
                % shouldn't happen when running normally, but could if cache functions
                % are called directly
                debug('Warning: Unable to determine whether analysis cache is valid because no .database found\n');

                % assume the cache is valid
                timestamp = -Inf;
            else
                % loop through these tables and find the latest modification time
                list = obj.getEntryNamesChangesInvalidateCache();

                % ask the database when the latest modification to these tables was
                timestamp = obj.database.getLastUpdated(list);
            end
        end

        function da = prepareForCache(da, varargin)
            p = inputParser;
            p.addParameter('snapshot', false, @islogical);
            p.addParameter('snapshotName', '', @ischar);
            p.KeepUnmatched = true;
            p.parse(varargin{:});

            if isempty(da.resultTable)
                error('Cannot cache DatabaseAnalysis before it has been initialized');
            end

%             if p.Results.snapshot
%                 % let the result table snapshot itself, then empty it
%                 da.resultTable.snapshot(p.Results.snapshotName);
%             else
%                 da.resultTable.cache();
%             end
%             da.resultTable = [];
        end

        function da = postLoadFromCache(da, param, timestamp, preLoadObj, varargin)
            p = inputParser;
            p.addParameter('snapshot', false, @islogical);
            p.addParameter('snapshotName', '', @ischar);
            p.KeepUnmatched = true;
            p.parse(varargin{:});

            if ~isempty(preLoadObj.database)
                da.setDatabase(preLoadObj.database);
            end

%             if p.Results.snapshot
%                 % let the result table snapshot itself, then empty it
%                 da.resultTable = da.resultTable.loadFromSnapshot(p.Results.snapshotName);
%             else
%                 % initialize if we need the result table to allow loading it from cache!
%                 if isempty(da.resultTable)
%                     da.initialize();
%                 end
%                 da.resultTable = da.resultTable.loadFromCache();
%             end

            if ~isempty(da.database)
                da.initialize();
            end
        end
    end

    methods % Dependent properties
        % path to folder of this run of this analysis
        function path = get.pathAnalysis(da)
            root = getFirstExisting(MatdbSettingsStore.settings.pathListAnalysis);
            name = da.getName();
            path = GetFullPath(fullfile(root, name, 'current'));
            if ~da.getIsRunning()
                return;
            else
                if isempty(da.timeRun) || isnan(da.timeRun)
                    timestr = 'current';
                else
                    timestr = datestr(da.timeRun, 'yyyy-mm-dd HH.MM.SS');
                end

                root = getFirstExisting(MatdbSettingsStore.settings.pathListAnalysis);
                name = da.getName();
                path = GetFullPath(fullfile(root, name, timestr));
            end
        end

        % path to parent folder of all runs of this analysis
        function path = get.pathAnalysisRoot(da)
            root = getFirstExisting(MatdbSettingsStore.settings.pathListAnalysis);
            name = da.getName();
            path = GetFullPath(fullfile(root, name));
        end

        function path = get.pathCurrent(da)
            root = getFirstExisting(MatdbSettingsStore.settings.pathListAnalysis);
            name = da.getName();
            path = GetFullPath(fullfile(root, name, 'current'));
        end

        function path = get.pathFigures(da)
            path = GetFullPath(fullfile(da.pathAnalysis, 'figures'));
        end

        function fields = get.fieldsAnalysis(da)
            fields = da.getFieldsAnalysisAsValueMap();
        end

        function htmlFile = get.htmlFile(da)
            name = da.getName();
            path = da.pathAnalysis;
            fname = sprintf('%s.html', name);
            htmlFile = GetFullPath(fullfile(path,fname));
        end
    end

    methods % DataSource instantiations
        % return a string describing this datasource
        function str = describe(da)
            str = da.getName();
        end

        % actually load this into the database, assume all dependencies have been loaded
        function loadInDatabase(da, database, varargin)
            p = inputParser();
            p.addParameter('verbose', false, @islogical);
            p.parse(varargin{:});
            
            da.database = database;
            da.initialize();

            % load success and run fields
            debug('%s: Loading specified analysis results fields\n', da.getName());
            fieldsToLoad = da.getFieldsToLoadOnDataSourceLoad();

            da.resultTable.loadFields(fieldsToLoad, 'verbose', p.Results.verbose).updateInDatabase();
            
            da.postLoadInDatabase();
        end

        function useAsResultTable(da, resultTable)
            % utilize DataTable resultTable as .resultTable, and mark this analysis thereby loaded
            % into the database. This allows other analyses / sources which require
            % this analysis to be loaded in the database to proceed while acting on a
            % specific results table as provided.
            %
            % TODO refactor this to avoid duplication in .run()

            % load all data sources
            db = da.database;
            if isempty(db)
                error('Please call .setDatabase(db)');
            end

            da.resultTable = resultTable;
            da.initialize();
            da.hasRun = true;
        end

        function deleteCache(da)
            if isempty(da.resultTable);
                r = DatabaseAnalysisResultsTable(da);
            else
                r = da.resultTable;
            end
            r.deleteCache();
        end
    end

    methods % Caching utilities
        function fileList = getCacheFileListForReadForEntry(da, iEntry, varargin)
            if isempty(da.resultTable);
                r = DatabaseAnalysisResultsTable(da);
            else
                r = da.resultTable;
            end
            fileList = r.getCacheFileListForReadForEntry(iEntry, varargin{:});
        end

        function fileList = getCacheFileForWriteForEntry(da, iEntry, varargin)
            if isempty(da.resultTable);
                r = DatabaseAnalysisResultsTable(da);
            else
                r = da.resultTable;
            end
            fileList = r.getCacheFileForWriteForEntry(iEntry, varargin{:});
        end
    end
    
    methods(Hidden)
        function setTableMapped(da, dt)
            %assert(~da.hasRun, 'Database cannot be changed after analysis has run');
            assert(isempty(dt) || isa(dt, 'DataTable'), 'Must be a DataTable instance');
            da.tableMapped = dt;
        end
        
        function setCurrentEntry(da, e)
            %assert(~da.hasRun, 'Database cannot be changed after analysis has run');
            assert(isempty(e) || isa(e, 'DataTable'), 'Must be a DataTable instance');
            da.currentEntry = e;
            da.figureInfoCurrentEntry = [];
        end
    end

    methods(Static) % Diary file related statics
        function setOutputLogStatus(varargin)
            persistent currentDiaryFile;
            p = inputParser;
            p.addParameter('file', '', @ischar);
            p.addParameter('paused', [], @islogical);
            p.parse(varargin{:});

            if ~isempty(p.Results.file)
                currentDiaryFile = p.Results.file;
            end

            if ~isempty(p.Results.paused)
                if p.Results.paused
                    diary off;
                else
                    if ~isempty(currentDiaryFile)
                        diary(currentDiaryFile);
                    end
                end
            end
        end

        function pauseOutputLog(da)
%             disp('off');
            DatabaseAnalysis.setOutputLogStatus('paused', true);
        end

        function resumeOutputLog(da)
%             disp('on')
            DatabaseAnalysis.setOutputLogStatus('paused', false);
        end
    end

end
